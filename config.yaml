# config.yaml
#model: models/qwen3-0.6b

# 1) vLLM serve 명령어 기본 인자
port: 8000
api_key: "token-abc123"
seed: 42

# 2) 모델 로딩 및 추론 정밀도 / 메모리 관련 옵션
dtype: "bfloat16"
kv_cache_dtype: "fp8_e5m2"
tensor_parallel_size: 8
gpu_memory_utilization: 0.9
swap_space: 4

# 3) RoPE(회전 위치 임베딩) 스케일링 설정
rope_scaling: "{\"rope_type\":\"yarn\",\"factor\":4.0,\"original_max_position_embeddings\":32768}"

# 4) 최대 컨텍스트 길이
max_model_len: 131072
max_seq_len_to_capture: 131072
#max_num_seqs: 8

# 5) 함수 호출(Function Calling) 관련 옵션
enable_auto_tool_choice: true
tool_call_parser: "hermes"

# 6) 원격 코드(trusted remote code) 허용 여부
trust_remote_code: true
